---
output: github_document
---

# web scraping

不同于API，需要自己爬虫。

rvest是主要负责爬虫的，但是实际上我需要知道 headers 这部分内容。

但是这里会介绍 rvest 包的标准使用方法，也值得借鉴。

```{r}
# Load rvest
library(rvest)

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Print test_xml
test_xml
```

熟悉的XML语言。

```{r}
test_node_xpath <- "//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]"
```

很复杂。

```{r}
node <- html_node(x = test_xml, xpath = test_node_xpath)
node
```

# HTML structure

`<a href = "https://en.wikipedia.org/"> this is a test </a>`

a - attr

1. `html_text(x = ___)` - get text contents
1. `html_attr(x = ___, name = ___)` - get specific attribute
1. `html_name(x = ___)` - get tag name

<input type="checkbox" id="checkbox1" class="styled">整合博客上的rvest

可以从网络上提取`table`

感觉如果是 `html_nodes` 这样的，含金量就不是很高了。

# HTML table

使用 `html_table`

<input type="checkbox" id="checkbox1" class="styled">[等下拿GDP](https://baike.sogou.com/v6172.htm?fromTitle=gdp)